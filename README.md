# neural-network-paper
A research paper on how learning rate, dropout, epochs, and optimizers affect CNN performance on MNIST.
# Neural Network Training Research

This project explores how learning rate, dropout rate, number of epochs, and optimizer choice affect the training of a convolutional neural network (CNN) on the MNIST dataset.

ðŸ“„ **[Click here to read the full research paper (PDF)](./Hongyi_NeuralNet_Paper.pdf)**

## ðŸ§  Summary

- **Model**: CNN trained on MNIST
- **Hyperparameters Tested**:
  - Learning Rate: 0.01, 0.001, 0.0001
  - Epochs: 5, 10, 20
  - Dropout: 0%, 20%, 50%
  - Optimizers: SGD, Adam, RMSprop
- **Key Findings**:
  - Small learning rates slow training
  - Dropout helps generalization but can hurt training speed
  - Adam optimizer yields fastest convergence and best accuracy

## ðŸ”¬ Project Files

- `Hongyi_NeuralNet_Paper.pdf` â€“ Final paper

## ðŸ“« Author

Hongyi Zhang  
Student at University High School, Irvine, CA  
